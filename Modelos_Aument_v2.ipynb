{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UWRuqWKp3c6a"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer,MinMaxScaler,LabelEncoder,OneHotEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tensorflow.keras.layers import concatenate\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ttest_ind\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import locale\n",
    "import glob\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S9h_9ytDkcKC"
   },
   "outputs": [],
   "source": [
    "def preprocesa_atributos(input_path):\n",
    "    \n",
    "    HOY = pd.to_datetime(datetime.date(2021,6,30)) # Fecha nominal para calcular los días de cobertura pendiente\n",
    "    '''\n",
    "    Dada la ruta a un archivo devuelve partición en conjuntos de entrenamiento y validación\n",
    "    input: ruta a un archivo, el que debe contener en una el link a la foto.\n",
    "    output: Conjuntos de entrenamiento y Validación, además de pd.Series con ruta a las imágenes\n",
    "    '''\n",
    "    \n",
    "    df=pd.read_csv(input_path, sep=';')\n",
    "    df.dropna(inplace=True)\n",
    "    df = df.drop(columns=['Condition']) # Queremos predecir el monto, por el momento no nos interesa\n",
    "    df = df[(df['Amount']<=df['Cost_of_vehicle'])] #Filtramos casos: Monto del daño no debiera ser > valor auto\n",
    "    df = df[df['Amount']>0] #Elimino vehículos sin daño\n",
    "    \n",
    "    df['Expiry_date']=pd.to_datetime(df['Expiry_date'])\n",
    "    df['dias_pendientes_cobertura']=(df['Expiry_date']-HOY).dt.days.astype(int) #Permite contar días hasta fin cobertura\n",
    "    df['relative_amount'] = df['Amount']/df['Cost_of_vehicle'] #Se expresa en términos relativos\n",
    "    df['relative_max_cov'] = df['Max_coverage']/df['Cost_of_vehicle'] #Se expresa en términos relativos\n",
    "    \n",
    "    \n",
    "    df = df.drop(columns=['Min_coverage','Expiry_date','Max_coverage','Amount'])\n",
    "    \n",
    "    scaler = MinMaxScaler() #Vamos a escalar variables numéricas\n",
    "     \n",
    "    X = df.drop(columns=['relative_amount'])\n",
    "    y = df['relative_amount']\n",
    "    \n",
    "    \n",
    "    #Revisar el test size, de manera que el conjunto de validación tenga un n usual (20%-30%)\n",
    "    \n",
    "    X_train,X_val,y_train,y_val = train_test_split(X, y, test_size=0.6)\n",
    "    \n",
    "    \n",
    "    \n",
    "    X_train_num = X_train[['Cost_of_vehicle','dias_pendientes_cobertura']]\n",
    "    X_val_num = X_val[['Cost_of_vehicle','dias_pendientes_cobertura']]\n",
    "    \n",
    "    X_train_num_scaled = pd.DataFrame(scaler.fit_transform(X_train_num), \n",
    "                                      columns=X_train_num.columns,\n",
    "                                      index=X_train.index)\n",
    "    \n",
    "    X_val_num_scaled = pd.DataFrame(scaler.transform(X_val_num), \n",
    "                                      columns=X_val_num.columns,\n",
    "                                       index=X_val.index)\n",
    "    \n",
    "    lb = LabelBinarizer()# Codificaremos valores categóricos\n",
    "    \n",
    "    \n",
    "    X_train_encoded = pd.DataFrame(lb.fit_transform(X_train['Insurance_company']),\n",
    "                                    columns=lb.classes_,\n",
    "                                   index=X_train.index)\n",
    "    \n",
    "    X_val_encoded = pd.DataFrame(lb.transform(X_val['Insurance_company']),\n",
    "                                    columns=lb.classes_,\n",
    "                                   index=X_val.index)\n",
    "    \n",
    "    \n",
    "    X_train_feat = pd.concat([X_train['relative_max_cov'],X_train_num_scaled,X_train_encoded],axis=1)\n",
    "    X_val_feat = pd.concat([X_val['relative_max_cov'],X_val_num_scaled,X_val_encoded],axis=1)\n",
    "    \n",
    "    X_train_im = X_train['Image_path']\n",
    "    X_val_im = X_val['Image_path']\n",
    "    \n",
    "    \n",
    "    return X_train_feat,X_train_im, X_val_feat,X_val_im,y_train,y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H276-d3FRJn7"
   },
   "outputs": [],
   "source": [
    "#X_train_feat,X_train_im, X_val_feat,X_val_im,y_train,y_val = preprocesa_atributos('train_original2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(X_train_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(X_val_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "662/(5*541+662)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aH_F8_H4mLgE"
   },
   "outputs": [],
   "source": [
    "def carga_imagenes(serie_fotos, inputPath):\n",
    "    \n",
    "    nuevas_imagenes = []\n",
    "    for foto in serie_fotos:\n",
    "        imagen = cv2.imread(inputPath+'/'+foto)\n",
    "        imagen = cv2.resize(imagen, (128, 128)) #Dejo todas las fotos en 128 x 128\n",
    "        nuevas_imagenes.append(imagen)\n",
    "    return np.array(nuevas_imagenes)/255.0 # Normalizo a factor de 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TaoW7Y7zmPF7"
   },
   "outputs": [],
   "source": [
    "def create_mlp(dim, regress=True):\n",
    "    # Perceptron multicapas muy simple para Redes neuronales\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_dim=dim, activation=\"relu\"))\n",
    "    model.add(Dense(1, activation=\"relu\"))\n",
    "\n",
    "    # ¿Hay nodo de regresión?\n",
    "    if regress:\n",
    "        model.add(Dense(1, activation=\"linear\"))\n",
    "\n",
    "    # Devuelve un modelo\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rv9dJagqmVxA"
   },
   "outputs": [],
   "source": [
    "def create_cnn(width, height, depth, filters=(16, 32, 64), regress=True):\n",
    "# Ojo: hay que cambiar orden de las dimensiones\n",
    "\n",
    "    inputShape = (height, width, depth)\n",
    "    chanDim = -1\n",
    "\n",
    "    # Definir input del modelos\n",
    "    inputs = Input(shape=inputShape)\n",
    "\n",
    "    # Loop para cada uno de los filtros\n",
    "    for (i, f) in enumerate(filters):\n",
    "    # Ojo con capas de convolución\n",
    "        if i == 0:\n",
    "            x = inputs\n",
    "\n",
    "    # CONV => RELU => BN => POOL\n",
    "    x = Conv2D(f, (3, 3), padding=\"same\")(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization(axis=chanDim)(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    # Aplanar nuevamente, luego FC => RELU => BN => DROPOUT\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(32)(x)\n",
    "    x = Dense(8)(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization(axis=chanDim)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    # Aplicar al final\n",
    "    x = Dense(1)(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    # ¿Hay nodo de regresión?\n",
    "    if regress:\n",
    "        x = Dense(1, activation=\"linear\")(x)\n",
    "\n",
    "    # construct the CNN\n",
    "    model = Model(inputs, x)\n",
    "\n",
    "    # return the CNN\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "wYV6FTJGmeu0",
    "outputId": "926efa11-600d-495a-fea6-ee2ab10e9c68"
   },
   "outputs": [],
   "source": [
    "resultados_modelo_solo_imagenes = []\n",
    "resultados_modelo_multi_input = []\n",
    "\n",
    "for i in range(100): #Voy a correr 100 veces el experimento\n",
    "    \n",
    "    \n",
    "    X_train_feat,X_train_im, X_val_feat,X_val_im,y_train,y_val = preprocesa_atributos('train_original2.csv')\n",
    "    \n",
    "    \n",
    "    Union_train = pd.concat([X_train_im, X_train_feat,y_train],axis=1)\n",
    "\n",
    "    x1=Union_train.copy()\n",
    "    x1['Image_path']=x1['Image_path'].str.replace('.jpg','_1.jpg')\n",
    "\n",
    "    x2=Union_train.copy()\n",
    "    x2['Image_path']=x2['Image_path'].str.replace('.jpg','_2.jpg')\n",
    "\n",
    "    x3=Union_train.copy()\n",
    "    x3['Image_path']=x3['Image_path'].str.replace('.jpg','_3.jpg')\n",
    "\n",
    "    x4=Union_train.copy()\n",
    "    x4['Image_path']=x4['Image_path'].str.replace('.jpg','_4.jpg')\n",
    "\n",
    "    x5=Union_train.copy()\n",
    "    x5['Image_path']=x5['Image_path'].str.replace('.jpg','_5.jpg')\n",
    "\n",
    "    Union_X_train = pd.concat([Union_train,x1,x2,x3,x4,x5], axis=0)\n",
    "\n",
    "    X_train_im = Union_X_train['Image_path']\n",
    "    X_train_feat = Union_X_train[['relative_max_cov','Cost_of_vehicle','dias_pendientes_cobertura','A',\n",
    "                                  'AA','AC','B','BB','BC','BQ','C','DA','O','RE']]\n",
    "    y_train = Union_X_train['relative_amount']\n",
    "    \n",
    "    \n",
    "    \n",
    "    imagenes_train = carga_imagenes(X_train_im,'Fotos_train_aum')\n",
    "    imagenes_val = carga_imagenes(X_val_im,'Fotos_train_aum')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    cnn1 = create_cnn(128, 128, 3)\n",
    "    cnn2 = create_cnn(128, 128, 3)\n",
    "    mlp = create_mlp(X_train_feat.shape[1])\n",
    "\n",
    "    opt = Adam(learning_rate=1e-3,decay = 1e-2) # Hiperparametros\n",
    "    callback = EarlyStopping(monitor='loss', patience=5) # No quiero gastar recursos innecesarios, con callback\n",
    "\n",
    "    cnn1.compile(loss=\"mae\", \n",
    "                  optimizer=opt)\n",
    "\n",
    "    #print(\"[INFO] Entrenando modelo con imágenes...\")\n",
    "    history_imagenes = cnn1.fit(\n",
    "        x=imagenes_train, \n",
    "        y=y_train.values,\n",
    "        validation_data=(imagenes_val, y_val.values),\n",
    "        epochs=500, \n",
    "        batch_size=16,\n",
    "        verbose=0,\n",
    "        callbacks = [callback])\n",
    "\n",
    "    # Listar la historia\n",
    "    #print(history_imagenes.history.keys())\n",
    "    # Generar gráficos\n",
    "    plt.plot(history_imagenes.history['loss'])\n",
    "    plt.plot(history_imagenes.history['val_loss'])\n",
    "    plt.title('Loss - Modelo imágenes')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    y_pred_img = cnn1.predict(imagenes_val)\n",
    "\n",
    "    error_promedio_imagenes = mean_absolute_error(y_val.values.flatten(),y_pred_img)\n",
    "    \n",
    "    resultados_modelo_solo_imagenes.append(error_promedio_imagenes)\n",
    "\n",
    "    #Y ahora entreno el modelo que utiliza como insumos datos tabulares y las fotos\n",
    "\n",
    "    combinedInput = concatenate([mlp.output, cnn2.output])\n",
    "    x = Dense(1, activation=\"relu\")(combinedInput)\n",
    "    x = Dense(1, activation=\"linear\")(x)\n",
    "\n",
    "    model = Model(inputs=[mlp.input, cnn2.input], outputs=x)\n",
    "\n",
    "    opt = Adam(lr=1e-3,decay = 1e-2) # Hiperparametros\n",
    "    callback = EarlyStopping(monitor='loss', patience=10) # No quiero gastar recursos innecesarios, con callback\n",
    "\n",
    "    model.compile(loss=\"mae\", \n",
    "                  optimizer=opt)\n",
    "\n",
    "    # Entrenamos el modelo\n",
    "    #print(\"[INFO] Entrenando modelo multi input...\")\n",
    "    history = model.fit(\n",
    "        x=[X_train_feat.values, imagenes_train], \n",
    "        y=y_train.values,\n",
    "        validation_data=([X_val_feat.values, imagenes_val], y_val.values),\n",
    "        epochs=100, \n",
    "        batch_size=16,\n",
    "        verbose=0,\n",
    "        callbacks = [callback])\n",
    "\n",
    "\n",
    "    # Listar la historia\n",
    "    #print(history.history.keys())\n",
    "    # Generar gráficos\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Loss - Modelo multi input')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    y_pred_mi = model.predict([X_val_feat.values, imagenes_val])\n",
    "\n",
    "    error_promedio_mi = mean_absolute_error(y_val.values.flatten(),y_pred_mi)\n",
    "    \n",
    "    resultados_modelo_multi_input.append(error_promedio_mi)\n",
    "    \n",
    "    \n",
    "    print(f\"El promedio de y en el conjunto de entrenamiento es {round(np.mean(y_train),4)}, y contiene {len(y_train)} casos\")\n",
    "    print(f\"El promedio de y en el conjunto de validación es {round(np.mean(y_val),4)}, y contiene {len(y_val)} casos\")\n",
    "    \n",
    "\n",
    "    print(f\"El error absoluto promedio del modelo que solo usa imágenes es {round(error_promedio_imagenes,4)}\")\n",
    "    print(f\"A su vez, el error absoluto promedio en el modelo multi-input es {round(error_promedio_mi,4)}.\")\n",
    "    print(f\"Iteración {i+1} finalizada\")\n",
    "    \n",
    "    del cnn1\n",
    "    del cnn2\n",
    "    del mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZwGvxny-dQ6o",
    "outputId": "6e8614c7-b6ec-486a-850c-0a1dc16f0f76"
   },
   "outputs": [],
   "source": [
    "imagenes_media = np.mean(resultados_modelo_solo_imagenes)\n",
    "multi_input_media = np.mean(resultados_modelo_multi_input)\n",
    "\n",
    "print(\"Promedio error solo imágenes:\",imagenes_media)\n",
    "print(\"Promedio error multi input:\",multi_input_media)\n",
    "\n",
    "\n",
    "imagenes_std = np.std(resultados_modelo_solo_imagenes)\n",
    "multi_input_std = np.std(resultados_modelo_multi_input)\n",
    "\n",
    "print(\"Desviación Estándar solo imágenes:\",imagenes_std )\n",
    "print(\"Desviación Estándar solo multi_input:\",multi_input_std)\n",
    "\n",
    "ttest,pval = ttest_ind(resultados_modelo_solo_imagenes,resultados_modelo_multi_input)\n",
    "\n",
    "print(\"p-value\",pval)\n",
    "if pval <0.05:\n",
    "  print(\"No aceptamos hipótesis nula, las medias debieran ser distintas\")\n",
    "else:\n",
    "  print(\"Aceptamos la hipótesis nula, las medias deben ser equivalentes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JeGrzK8NdT6C"
   },
   "outputs": [],
   "source": [
    "compara = pd.DataFrame(zip(resultados_modelo_multi_input,resultados_modelo_solo_imagenes),\n",
    "      columns=['error_solo_imagen','error_multi_input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LwNErzrpdWAW"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "uYd6g_vFdZRS",
    "outputId": "77cb4daf-8439-4dc6-cf3d-b1a6f2628e87"
   },
   "outputs": [],
   "source": [
    "sns.pairplot(compara, diag_kind='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C1Og0pCCdbW6"
   },
   "outputs": [],
   "source": [
    "compara.to_csv('resultados.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Modelos_Aument.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
